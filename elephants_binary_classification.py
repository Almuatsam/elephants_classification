# -*- coding: utf-8 -*-
"""Elephants_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iSvnVlrWSD9onPV0fU2_4G4w661c0_6E

# Elephants Binary Classification using Deep Learning

This notebook implements a binary image classification solution for detecting African and Asian elephants using images. We'll compare three different models:
1. Basic CNN model
2. Improved CNN with BatchNormalization
3. Pre-trained ResNet50 model

The dataset contains elephants images classified as either African or Asian.

## Setup and Import Libraries

First, let's import all the necessary libraries and set up our environment.
"""

from tensorflow import keras
from keras import Sequential
from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,BatchNormalization,UpSampling2D, Dropout
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam
from google.colab import drive
import numpy as np
import os
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# Mounting the drive where the dataset resides
drive.mount('/content/drive')

# Define paths to the dataset
train_dir = '/content/drive/MyDrive/Elephants/train'  # training path
test_dir = '/content/drive/MyDrive/Elephants/test'    # testing path

# Print dataset statistics
print("Dataset Statistics:")
print(f"Training African Elephants images: {len(os.listdir('/content/drive/MyDrive/Elephants/train/African'))}")
print(f"Training Asian Elephants images: {len(os.listdir('/content/drive/MyDrive/Elephants/train/Asian'))}")
print(f"Test African Elephants images: {len(os.listdir('/content/drive/MyDrive/Elephants/test/African'))}")
print(f"Test Asian Elephants images: {len(os.listdir('/content/drive/MyDrive/Elephants/test/Asian'))}")

# Plot class distribution
categories = ['Train', 'Test']
african_counts = [len(os.listdir('/content/drive/MyDrive/Elephants/train/African')), len(os.listdir('/content/drive/MyDrive/Elephants/test/African'))]
asian_counts = [len(os.listdir('/content/drive/MyDrive/Elephants/train/Asian')), len(os.listdir('/content/drive/MyDrive/Elephants/test/Asian'))]

x = np.arange(len(categories))
width = 0.35

plt.figure(figsize=(10, 6))
plt.bar(x - width/2, african_counts, width, label='African')
plt.bar(x + width/2, asian_counts, width, label='Asian')

plt.xlabel('Dataset Split')
plt.ylabel('Number of Images')
plt.title('Class Distribution Across Dataset Splits')
plt.xticks(x, categories)
plt.legend()

# Add count labels on bars
for i, v in enumerate(african_counts):
    plt.text(i - width/2, v + 5, str(v), ha='center')
for i, v in enumerate(asian_counts):
    plt.text(i + width/2, v + 5, str(v), ha='center')

plt.tight_layout()
plt.show()

"""## Data Preprocessing and Augmentation

Now, let's preprocess the data and set it up for training.
"""

# Create ImageDataGenerator for training set
train_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2  # Split 20% of the images for validation
)

# Load and prepare training data
train_data = train_datagen.flow_from_directory(
    train_dir,
    target_size=(256,256),
    batch_size=32,
    class_mode='binary',  # 'binary' for binary classification
    subset='training'  # Specify 'training' for the training set
)

# Create ImageDataGenerator for validation set
validation_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2  # Note: Using the same validation split as in the training set
)

# Load and prepare validation data
validation_data = validation_datagen.flow_from_directory(
    train_dir,
    target_size=(256,256),
    batch_size=32,
    class_mode='binary',
    subset='validation'  # Specify 'validation' for the validation set
)

"""## Model 1: Basic CNN Model

Let's implement and train a basic CNN model for skin cancer classification.
"""

# create CNN model
print("\n--- Training Basic CNN Model ---\n")

basic_model = Sequential()

basic_model.add(Conv2D(32,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(256,256,3)))  # 32 filters
basic_model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

basic_model.add(Conv2D(64,kernel_size=(3,3),padding='valid',activation='relu'))
basic_model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

basic_model.add(Conv2D(128,kernel_size=(3,3),padding='valid',activation='relu'))
basic_model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

basic_model.add(Flatten())

basic_model.add(Dense(128,activation='relu')) #feature reduction
basic_model.add(Dense(64,activation='relu'))
basic_model.add(Dense(1,activation='sigmoid'))  #output layer

# Display model summary
basic_model.summary()

# Train the model
basic_model.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy']) #binary_crossentropy - binary classification
basic_history = basic_model.fit(train_data, epochs=20, validation_data=validation_data)

# Plot training history - Accuracy
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(basic_history.history['accuracy'], color='red', label='train')
plt.plot(basic_history.history['val_accuracy'], color='blue', label='validation')
plt.title('Basic CNN Model - Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot training history - Loss
plt.subplot(1, 2, 2)
plt.plot(basic_history.history['loss'], color='red', label='train')
plt.plot(basic_history.history['val_loss'], color='blue', label='validation')
plt.title('Basic CNN Model - Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_datagen = ImageDataGenerator(rescale=1./255)
test_data = test_datagen.flow_from_directory(
    test_dir,
    target_size=(256,256),
    batch_size=32,
    class_mode='binary'
)

# Evaluate on test data
basic_predictions = basic_model.predict(test_data)
basic_predicted_labels = (basic_predictions > 0.5).astype(int)

# Calculate confusion matrix and classification report
basic_cm = confusion_matrix(test_data.classes, basic_predicted_labels)
basic_cr = classification_report(test_data.classes, basic_predicted_labels)

print("\nBasic CNN Model - Confusion Matrix:")
print(basic_cm)
print("\nBasic CNN Model - Classification Report:")
print(basic_cr)

"""## Model 2: Improved CNN Model with BatchNormalization

Now, let's implement and train an improved CNN model with BatchNormalization to reduce overfitting.
"""

print("\n--- Training Improved CNN Model with BatchNormalization ---\n")

improved_model = Sequential()

improved_model.add(Conv2D(32,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(256,256,3)))  # 32 filters
improved_model.add(BatchNormalization())  # added to reduce overfitting
improved_model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

improved_model.add(Conv2D(64,kernel_size=(3,3),padding='valid',activation='relu'))
improved_model.add(BatchNormalization())  # added to reduce overfitting
improved_model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

improved_model.add(Conv2D(128,kernel_size=(3,3),padding='valid',activation='relu'))
improved_model.add(BatchNormalization())  # added to reduce overfitting
improved_model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

improved_model.add(Flatten())

improved_model.add(Dense(128,activation='relu')) #feature reduction
improved_model.add(Dropout(0.1))  # added to reduce overfitting
improved_model.add(Dense(64,activation='relu'))
improved_model.add(Dropout(0.1))  # added to reduce overfitting
improved_model.add(Dense(1,activation='sigmoid'))  #output layer


improved_model.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])
improved_history = improved_model.fit(train_data, epochs=20, validation_data=validation_data)

# Plot training history - Accuracy
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(improved_history.history['accuracy'], color='red', label='train')
plt.plot(improved_history.history['val_accuracy'], color='blue', label='validation')
plt.title('Improved CNN Model - Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot training history - Loss
plt.subplot(1, 2, 2)
plt.plot(improved_history.history['loss'], color='red', label='train')
plt.plot(improved_history.history['val_loss'], color='blue', label='validation')
plt.title('Improved CNN Model - Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_datagen = ImageDataGenerator(rescale=1./255)
test_data = test_datagen.flow_from_directory(
    test_dir,
    target_size=(256,256),
    batch_size=32,
    class_mode='binary'
)


predictions = improved_model.predict(test_data)

# Evaluate on test data
improved_predictions = improved_model.predict(test_data)
improved_predicted_labels = (improved_predictions > 0.5).astype(int)

# Calculate confusion matrix and classification report
improved_cm = confusion_matrix(test_data.classes, improved_predicted_labels)
improved_cr = classification_report(test_data.classes, improved_predicted_labels)

print("\nImproved CNN Model - Confusion Matrix:")
print(improved_cm)
print("\nImproved CNN Model - Classification Report:")
print(improved_cr)

"""## Model 3: Pre-trained ResNet50 Model with Transfer Learning

Finally, let's implement and train a pre-trained ResNet50 model.
"""

print("\n--- Training Pre-trained ResNet50 Model ---\n")

resnet_model = Sequential()    #ResNet50 is a pre-trained model
pretrained_model = tf.keras.applications.ResNet50(
    include_top = False, #because i have my own image of a diff dimension
    input_shape = (256,256,3),
    pooling = 'max', classes = 2,
    weights = 'imagenet')

# Freeze the pre-trained layers
for layer in pretrained_model.layers:
    layer.trainable = False

resnet_model.add(pretrained_model)
resnet_model.add(Flatten())
resnet_model.add(Dense(512, activation = 'relu'))
resnet_model.add(Dense(1, activation = 'sigmoid'))


resnet_model.compile(optimizer=Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])

# Display model summary
resnet_model.summary()

resnet_history = resnet_model.fit(train_data, epochs=20, validation_data=validation_data)

# Plot training history - Accuracy
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(resnet_history.history['accuracy'], color='red', label='train')
plt.plot(resnet_history.history['val_accuracy'], color='blue', label='validation')
plt.title('ResNet50 Model - Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot training history - Loss
plt.subplot(1, 2, 2)
plt.plot(resnet_history.history['loss'], color='red', label='train')
plt.plot(resnet_history.history['val_loss'], color='blue', label='validation')
plt.title('ResNet50 Model - Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate on test data
resnet_predictions = resnet_model.predict(test_data)
resnet_predicted_labels = (resnet_predictions > 0.5).astype(int)

# Calculate confusion matrix and classification report
resnet_cm = confusion_matrix(test_data.classes, resnet_predicted_labels)
resnet_cr = classification_report(test_data.classes, resnet_predicted_labels)

print("\nResNet50 Model - Confusion Matrix:")
print(resnet_cm)
print("\nResNet50 Model - Classification Report:")
print(resnet_cr)

"""## Model Performance Comparison

Let's compare the performance of all three models using various evaluation metrics.
"""

print("\n--- Model Performance Comparison ---\n")

# Function to extract metrics from classification report
def extract_metrics(report_text):
    lines = report_text.strip().split('\n')
    # Find the line with weighted avg
    for line in lines:
        if 'weighted avg' in line:
            parts = line.split()
            precision = float(parts[2])
            recall = float(parts[3])
            f1 = float(parts[4])
            return precision, recall, f1
    return None, None, None

# Extract metrics
basic_precision, basic_recall, basic_f1 = extract_metrics(basic_cr)
improved_precision, improved_recall, improved_f1 = extract_metrics(improved_cr)
resnet_precision, resnet_recall, resnet_f1 = extract_metrics(resnet_cr)

# Calculate accuracy from confusion matrix
basic_accuracy = np.trace(basic_cm) / np.sum(basic_cm)
improved_accuracy = np.trace(improved_cm) / np.sum(improved_cm)
resnet_accuracy = np.trace(resnet_cm) / np.sum(resnet_cm)

# Create comparison plot
models = ['Basic CNN', 'Improved CNN', 'ResNet50']
accuracy = [basic_accuracy, improved_accuracy, resnet_accuracy]
precision = [basic_precision, improved_precision, resnet_precision]
recall = [basic_recall, improved_recall, resnet_recall]
f1 = [basic_f1, improved_f1, resnet_f1]

x = np.arange(len(models))
width = 0.2

plt.figure(figsize=(12, 6))
plt.bar(x - width*1.5, accuracy, width, label='Accuracy')
plt.bar(x - width/2, precision, width, label='Precision')
plt.bar(x + width/2, recall, width, label='Recall')
plt.bar(x + width*1.5, f1, width, label='F1-Score')

plt.xlabel('Models')
plt.ylabel('Score')
plt.title('Model Performance Comparison')
plt.xticks(x, models)
plt.legend()
plt.ylim(0, 1.0)

# Add value labels
for i, v in enumerate(accuracy):
    plt.text(i - width*1.5, v + 0.02, f'{v:.2f}', ha='center', va='bottom', fontsize=8)
for i, v in enumerate(precision):
    plt.text(i - width/2, v + 0.02, f'{v:.2f}', ha='center', va='bottom', fontsize=8)
for i, v in enumerate(recall):
    plt.text(i + width/2, v + 0.02, f'{v:.2f}', ha='center', va='bottom', fontsize=8)
for i, v in enumerate(f1):
    plt.text(i + width*1.5, v + 0.02, f'{v:.2f}', ha='center', va='bottom', fontsize=8)

plt.tight_layout()
plt.show()

# Create a comparison table
import pandas as pd

comparison_df = pd.DataFrame({
    'Model': models,
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-Score': f1
})

print("Model Performance Comparison Table:")
print(comparison_df.round(4))

print("\nAll models trained and evaluated successfully!")